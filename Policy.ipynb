{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import helicopter\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as st\n",
    "import time\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITERABLE_STEPS():\n",
    "    def __init__(self,env, H, actions):\n",
    "        self.enviroment=env\n",
    "        self.H_ref = H      \n",
    "        self.actions = actions\n",
    "        self.index = 0\n",
    "        self.TOP = len(actions)\n",
    "        return None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index == self.TOP:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            i = self.index\n",
    "            self.index += 1\n",
    "            return (Copy(self.enviroment), self.actions[i], self.H_ref)     \n",
    "\n",
    "    def __del__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():    \n",
    "    def __init__(self, ):\n",
    "        self.policy = dict()\n",
    "\n",
    "    def new(self,key,control,value):\n",
    "        print(key)\n",
    "        try:\n",
    "            # The key state is already seen\n",
    "            controls, values, actions_freq , total_freq = self.policy[key]\n",
    "            # Iteration over controls available in key until reach it\n",
    "            if control in controls:\n",
    "                print(\"Estado con acción ya escogida\")\n",
    "                print(self.policy[key])\n",
    "                i = 0\n",
    "                for j in controls:\n",
    "                    if j == control:\n",
    "                        break\n",
    "                    else:\n",
    "                        i += 1\n",
    "                values[i]=(values[i]+value)/2\n",
    "                actions_freq[i]+=1                               \n",
    "            #If state has not this control, then add it with actual reward and update freq\n",
    "            else:\n",
    "                print(\"Estado con acción aún no vista\")\n",
    "                print(self.policy[key])\n",
    "                controls.append(control)\n",
    "                values.append(value)\n",
    "                actions_freq.append(1)\n",
    "            total_freq+=1\n",
    "            self.policy[key] = (controls,values, actions_freq, total_freq)\n",
    "            print(self.policy[key])\n",
    "        except:\n",
    "            # The key state is new, so their values are created\n",
    "            #Set of controls, Set of q_values,freq of choosing this control, total freq of being in this state\n",
    "            print(\"Nuevo estado agregado\")            \n",
    "            self.policy[key] = ([control],[value],[1],1) \n",
    "            print(self.policy[key])\n",
    "            \n",
    "    def __repr__(self):\n",
    "        s = \"Q-Table with {0} states\".format(len(self.policy.keys()))\n",
    "        for key in self.policy.keys():\n",
    "            s+=\"\\nState {0} controls:{1}\".format(key,self.policy[key][0])\n",
    "        return s\n",
    "    \n",
    "    def call(self, key,mode):        \n",
    "        try: \n",
    "            print(\"Modo:\",mode)\n",
    "            print(\"State:\",key)\n",
    "            (controls,q_values,freqs,total_freq)= self.policy[key]\n",
    "            print(\"Encontre el estado:\",self.policy[key])\n",
    "            if mode==\"stochastic\":\n",
    "                print(\"Seleccion estocastica de controles\")\n",
    "                freq_r = np.array(freqs) / total_freq\n",
    "                action_max=np.random.choice(controls, 1, p=freq_r)[0]\n",
    "                return action_max\n",
    "            elif mode==\"deterministic\":\n",
    "                print(\"selección determinista de controles\")\n",
    "                action_max= controls[q_values.index(max(q_values))]\n",
    "                return  action_max\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, H,observation,K,A,N_SAMPLES,vision,epsilon):  \n",
    "    grid, pos, remain_steps= observation\n",
    "    actions= env.available_actions(pos)   \n",
    "    NEXT_STEPS=ITERABLE_STEPS(env,H,actions)\n",
    "    q_values={}\n",
    "    for action_step in NEXT_STEPS:       \n",
    "        action_s, q_value= Simulation(action_step,K,A,N_SAMPLES,vision)        \n",
    "        q_values[action_s]=q_value   \n",
    "    if random.random() > epsilon:\n",
    "        print(\"Explotation\")\n",
    "        #Maximization over all possible action controls (max u_xk)\n",
    "        action_max= max(q_values.keys(), key=(lambda k: q_values[k]))\n",
    "    else:\n",
    "        print(\"Exploration\")\n",
    "        action_max=random.choice(list(q_values.keys()))   \n",
    "    return(action_max,q_values[action_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(simulation_args,K,A,N_SAMPLES,vision):    \n",
    "    total_q_value=0\n",
    "    alpha=A\n",
    "    env_c, action, H = simulation_args  \n",
    "    #Make first step(1-step-lookahead deterministic)\n",
    "    observation, g_k, done, info = env_c.step(action)\n",
    "    checkpoint = env_c.make_checkpoint()\n",
    "    total_q_value=0   \n",
    "    #Follow the heuristic K-steps   \n",
    "    for n_samples in range(N_SAMPLES):        \n",
    "        t_cost=0\n",
    "        q_value=0        \n",
    "        env_c.load_checkpoint(checkpoint)      \n",
    "        for r_iter in range(K):                \n",
    "                action_H = H(observation,vision) \n",
    "                observation, cost, done, info = env_c.step(action_H)                \n",
    "                cost=cost*alpha \n",
    "                alpha=alpha*A\n",
    "                t_cost=t_cost + cost               \n",
    "        q_value=g_k+t_cost #q-value for one sample\n",
    "        total_q_value=total_q_value+q_value #Adding up values of N_SAMPLES               \n",
    "    total_q_value=total_q_value/N_SAMPLES #Average of q value cost over all samples   \n",
    "    del env_c\n",
    "    return (action,total_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Heuristic(observation,vision):\n",
    "    grid, pos, remain_steps= observation\n",
    "    \n",
    "    #Add borders in Grid perimeter according to vision range of helicopter, in order\n",
    "    #to explore without boundary limits (Example: 2 vision add 2 cells at each side of Grid)\n",
    "    Pad_grid=ExpandGrid(grid,vision)    \n",
    "    \n",
    "    #Get neighborhood in agent current position and vision range\n",
    "    neighborhood= get_neighborhood(Pad_grid,pos,vision) \n",
    "    \n",
    "    #Count fire cells by zone(8 zones)    \n",
    "    burned_densities={}\n",
    "    \n",
    "    #Up Zone\n",
    "    up_zone=neighborhood[ 0:neighborhood.shape[0]-(vision+1),0:neighborhood.shape[1]] #Get Up Zone\n",
    "    up_burned=Count_Burned_Trees(env,up_zone) #Get fire cells in up zone    \n",
    "    burned_densities[\"up\"]=up_burned #Add zone and fire density to dictionary\n",
    "    \n",
    "    #Up Left Zone\n",
    "    up_left_zone=neighborhood[ 0:neighborhood.shape[0]-(vision+1),0:neighborhood.shape[0]-(vision+1) ]\n",
    "    up_left_burned=Count_Burned_Trees(env,up_zone)   \n",
    "    burned_densities[\"up_left\"]=up_left_burned    \n",
    "    \n",
    "    #Up Right Zone\n",
    "    up_right_zone=neighborhood[ 0:neighborhood.shape[0]-(vision+1),neighborhood.shape[0]-vision:neighborhood.shape[0] ]\n",
    "    up_right_burned=Count_Burned_Trees(env,up_right_zone)   \n",
    "    burned_densities[\"up_right\"]=up_right_burned    \n",
    "    \n",
    "    #Down Zone\n",
    "    down_zone=neighborhood[ neighborhood.shape[0]-vision:neighborhood.shape[0],0:neighborhood.shape[1]]\n",
    "    down_burned=Count_Burned_Trees(env,down_zone)    \n",
    "    burned_densities[\"down\"]=down_burned\n",
    "    \n",
    "    #Down Left\n",
    "    down_left_zone=neighborhood[ neighborhood.shape[0]-vision:neighborhood.shape[0], 0:neighborhood.shape[0]-(vision+1) ]\n",
    "    down_left_burned=Count_Burned_Trees(env,down_left_zone)    \n",
    "    burned_densities[\"down_left\"]=down_left_burned    \n",
    "    \n",
    "    #Down Right\n",
    "    down_right_zone=neighborhood[ neighborhood.shape[0]-vision:neighborhood.shape[0], neighborhood.shape[0]-vision:neighborhood.shape[0] ]\n",
    "    down_right_burned=Count_Burned_Trees(env,down_right_zone)   \n",
    "    burned_densities[\"down_right\"]=down_right_burned   \n",
    "    \n",
    "    #Left Zone\n",
    "    left_zone=neighborhood[ 0:neighborhood.shape[0],0:neighborhood.shape[0]-(vision+1)]\n",
    "    left_burned=Count_Burned_Trees(env,left_zone)    \n",
    "    burned_densities[\"left\"]=left_burned\n",
    "    \n",
    "    #Right Zone\n",
    "    right_zone=neighborhood[ 0:neighborhood.shape[1],neighborhood.shape[0]-vision:neighborhood.shape[0]]\n",
    "    right_burned=Count_Burned_Trees(env,right_zone)   \n",
    "    burned_densities[\"right\"]=right_burned\n",
    "    \n",
    "    #Action based on burned trees/zone\n",
    "    actions= ((1,2,3),\n",
    "              (4,5,6),\n",
    "              (7,8,9))\n",
    "    \n",
    "    #Max function will return a (key,value) tuple of the maximum value from the dictionary\n",
    "    mx_tuple = max(burned_densities.items(),key = lambda x:x[1]) \n",
    "    #Mx_tuple[1] indicates maximum dictionary items value\n",
    "    max_list =[i[0] for i in burned_densities.items() if i[1]==mx_tuple[1]] \n",
    "    \n",
    "    #Apply Heuristic Rules according to fire cells in each zone\n",
    "    #If there are more than 1 max burn zone, choose randomly\n",
    "    if len(max_list) > 1: \n",
    "        a=random.choice(max_list)\n",
    "        if a==\"up\":\n",
    "            action=actions[0][1]\n",
    "        elif a==\"down\":\n",
    "            action=actions[2][1]\n",
    "        elif a==\"left\":\n",
    "            action=actions[1][0]\n",
    "        elif a==\"right\":\n",
    "            action=actions[1][2]\n",
    "        elif a==\"up_left\":\n",
    "            action=actions[0][0]\n",
    "        elif a==\"up_right\":\n",
    "            action=actions[0][2]\n",
    "        elif a==\"down_left\":\n",
    "            action=actions[2][0]\n",
    "        elif a==\"down_right\":\n",
    "            action=actions[2][2]\n",
    "    #If there is only one zone with max fire density (move in up,down,right,left or corners only)\n",
    "    elif len(max_list)==1:\n",
    "        if max_list[0]==\"up\":\n",
    "            action=actions[0][1]\n",
    "        elif max_list[0]==\"down\":\n",
    "            action=actions[2][1]\n",
    "        elif max_list[0]==\"left\":\n",
    "            action=actions[1][0]\n",
    "        elif max_list[0]==\"right\":\n",
    "            action=actions[1][2]\n",
    "        elif max_list[0]==\"up_left\":\n",
    "            action=actions[0][0]\n",
    "        elif max_list[0]==\"up_right\":\n",
    "            action=actions[0][2]\n",
    "        elif max_list[0]==\"down_left\":\n",
    "            action=actions[2][0]\n",
    "        elif max_list[0]==\"down_right\":\n",
    "            action=actions[2][2]\n",
    "        else:\n",
    "            action=random.randint(1, 9)\n",
    "    act=action        \n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a grid zone and count fire cells\n",
    "def Count_Burned_Trees(env,zone):\n",
    "    counter=0\n",
    "    for row in range(zone.shape[0]):\n",
    "        for col in range(zone.shape[1]):\n",
    "            if zone[row][col]==env.fire:\n",
    "                counter+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get neighborhood of agent according to vision range\n",
    "def get_neighborhood(grid,pos,vision):\n",
    "    pos_row=pos[0]\n",
    "    pos_col=pos[1]    \n",
    "    neighborhood=grid[pos_row:pos_row+1+vision*2,pos_col:pos_col+1+vision*2]\n",
    "    return neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpandGrid(grid,vision):        \n",
    "        size = grid.shape        \n",
    "        PadGrid = np.zeros((size[0],size[1]), dtype=np.int16)        \n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                if(grid[i][j][0]==1):\n",
    "                    PadGrid[i][j]=0\n",
    "                elif(grid[i][j][1]==1):\n",
    "                    PadGrid[i][j]=1\n",
    "                else:\n",
    "                    PadGrid[i][j]=2\n",
    "        size=PadGrid.shape\n",
    "        PadGrid2 = np.zeros((size[0]+2*vision,size[1]+2*vision), dtype=np.int16)\n",
    "        PadGrid2[vision:-vision,vision:-vision] = PadGrid\n",
    "        return PadGrid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Copy(env):\n",
    "    n_env = helicopter.EnvMakerForestFire(init_pos_row=env.pos_row,init_pos_col=env.pos_col,n_row = env.n_row, n_col = env.n_col,\n",
    "                                          p_tree = env.p_tree, p_fire =env.p_fire, moves_before_updating = env.moves_before_updating,\n",
    "                                          reward_type = env.reward_type, reward_tree = env.reward_tree,reward_fire = env.reward_fire,\n",
    "                                          reward_empty =env.reward_empty, reward_hit = env.reward_hit,sub_tree = env.sub_tree,\n",
    "                                          sub_empty = env.sub_empty, sub_fire = env.sub_fire, sub_rock = env.sub_rock,sub_lake = env.sub_lake,\n",
    "                                          ip_tree = env.ip_tree, ip_empty =env.ip_empty, ip_fire =env.ip_fire, ip_rock = env.ip_rock,\n",
    "                                          ip_lake = env.ip_lake)\n",
    "    n_env.grid = copy.deepcopy(env.grid)      \n",
    "    n_env.total_reward = copy.deepcopy(env.total_reward)   \n",
    "    n_env.total_hits=copy.deepcopy(env.total_hits)\n",
    "    n_env.remaining_moves=copy.deepcopy(env.remaining_moves)\n",
    "    \n",
    "    return n_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "N_ROW = 16\n",
    "N_COL = 16\n",
    "Init_Row=7\n",
    "Init_Col=7\n",
    "P_FIRE = 0.03\n",
    "P_TREE = 0.1\n",
    "# Symbols for cells\n",
    "TREE = 0\n",
    "FIRE = 2\n",
    "EMPTY = 1\n",
    "FREEZE = 8 #Movements of Helicopter after update Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Create a new enviroment with initial parameters\n",
    "    env = helicopter.EnvMakerForestFire(n_row = N_ROW, n_col = N_COL, p_tree = P_TREE, p_fire = P_FIRE,\n",
    "                 init_pos_row = Init_Row, init_pos_col = Init_Col, moves_before_updating = FREEZE,  \n",
    "                 tree = TREE, empty = EMPTY, fire = FIRE)  \n",
    "  \n",
    "    # First observation\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #Create a copy of enviroment with initial observation\n",
    "    env_1 = Copy(env)\n",
    "    observation_1 = observation   \n",
    "    \n",
    "    # Making checkpoints\n",
    "    checkpoint_env = env.make_checkpoint()\n",
    "    checkpoint_env_1 = env_1.make_checkpoint()\n",
    "    \n",
    "    #Create a new empty Policy     \n",
    "    policy= Policy()\n",
    "    \n",
    "    #Rollout Variables\n",
    "    N_TEST= 20      #Number of Training total simulations of Rollout\n",
    "    N_STEPS=50      #Number of steps in rollout(50 updates of enviroment and 20*8 movements of agent)\n",
    "    A=0.9           #Discount factor for future rewards\n",
    "    K=10            #Rollout Steps on horizon for the heuristic \n",
    "    N_SAMPLES=20    #Number of samples trajectories in rollout to calculate expected value\n",
    "    vision= 1       #Range to lookup in cells in helicpter heuristic  \n",
    "    epsilon=0.99    #Epsilon for exploration in state space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    RO_RESULTS=[]\n",
    "    H_RESULTS=[]\n",
    "    RO_RESULTS_C=[]\n",
    "    H_RESULTS_C=[]\n",
    "    start = time.time()\n",
    "    for n_test in range(N_TEST):        \n",
    "        print(\"Test:\",n_test)           \n",
    "        env.load_checkpoint(checkpoint_env)\n",
    "        env_1.load_checkpoint(checkpoint_env_1) \n",
    "        rollout_cost=0\n",
    "        heuristic_cost=0\n",
    "        rollout_cost_step=[]\n",
    "        heuristic_cost_step=[]        \n",
    "        for i in tqdm.tqdm(range(FREEZE * N_STEPS)):\n",
    "            print(\"Step:\",i)\n",
    "            #env.render()\n",
    "            r_action, q_value=rollout(env,Heuristic,observation,K,A,N_SAMPLES,vision,epsilon)\n",
    "            epsilon=epsilon*0.99\n",
    "            h_action=Heuristic(observation_1,vision)\n",
    "            #Update Policy            \n",
    "            policy.new(env.Encode(),r_action,q_value)            \n",
    "            ###############\n",
    "\n",
    "            print(\"Rollout Action:\",r_action)\n",
    "            print(\"Heuristic Action:\",h_action)\n",
    "            #Next Steps\n",
    "            observation, ro_cost, _, _ = env.step(r_action)\n",
    "            observation_1, h_cost, _, _ = env_1.step(h_action)  \n",
    "\n",
    "            rollout_cost += ro_cost  #Acumulative cost for rollout          \n",
    "            rollout_cost_step.append(rollout_cost)  #List of cost over time\n",
    "\n",
    "            heuristic_cost += h_cost\n",
    "            heuristic_cost_step.append(heuristic_cost)\n",
    "\n",
    "            print(\"Rollout in step {} is: {}\".format(i,rollout_cost))\n",
    "            print(\"Heuristic in step {} is: {}\".format(i,heuristic_cost))\n",
    "        print(\"Rollout:\",rollout_cost)\n",
    "        print(\"heuristic:\",heuristic_cost)\n",
    "        #Costs p/test\n",
    "        RO_RESULTS.append(rollout_cost)             \n",
    "        H_RESULTS.append(heuristic_cost)\n",
    "        #Cumulative costs p/test\n",
    "        RO_RESULTS_C.append(rollout_cost_step)\n",
    "        H_RESULTS_C.append(heuristic_cost_step)\n",
    "    print(\"Total time execution %.3f s\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(RO_RESULTS_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #OBTAIN AVERAGE COSTS PER STAGE\n",
    "    ITER=[]\n",
    "    ITER2=[]\n",
    "    RO_RESULTS_C=np.array(RO_RESULTS_C)    \n",
    "    for i in range(RO_RESULTS_C.shape[1]):\n",
    "        IT=[]\n",
    "        IT2=[]\n",
    "        for j in range(RO_RESULTS_C.shape[0]):\n",
    "            IT.append(RO_RESULTS_C[j][i])\n",
    "            IT2.append(H_RESULTS_C[j][i])\n",
    "        ITER.append(st.mean(IT))\n",
    "        ITER2.append(st.mean(IT2))        \n",
    "    \n",
    "    x = np.arange(RO_RESULTS_C.shape[1])    \n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average reward over 20 test (1 step-1 vision)')\n",
    "    plt.plot(x,ITER ,label='Rollout')\n",
    "    plt.plot(x,ITER2, label='Heuristic')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Grapg of total cost by test\n",
    "    RO_RESULTS = np.array(RO_RESULTS)    \n",
    "    H_RESULTS = np.array(H_RESULTS) \n",
    "    RO_RESULTS_MEAN=[]    \n",
    "    RO_RESULTS_MEAN.append(st.mean(np.array(RO_RESULTS_C[0])))   \n",
    "    x = np.arange(20)    \n",
    "    plt.xlabel('Test')\n",
    "    plt.ylabel('Final Total Reward (1 step- 1 vision)')\n",
    "    plt.title('Rollout Tests')\n",
    "    plt.plot(x,RO_RESULTS ,label='Rollout')\n",
    "    plt.plot(x,H_RESULTS, label='Heuristic')\n",
    "    plt.legend()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Simulation of enviroment with Trained Policy    \n",
    "    observation = env.reset()\n",
    "    env_1 = env.Copia()\n",
    "    observation_=observation\n",
    "    fig=env.render()\n",
    "    fig.savefig('Pictures/Env.png')\n",
    "    total_reward = 0\n",
    "    total_reward_step=[]\n",
    "    total_reward_h = 0\n",
    "    total_reward_step_h=[]   \n",
    "    N_STEPS=50\n",
    "    for j in tqdm.tqdm(range(N_STEPS*FREEZE)):\n",
    "        p_action=policy.call(env.Encode(),\"stochastic\")\n",
    "        print(env.Encode())\n",
    "        h_action=Heuristic(observation_,vision)\n",
    "        if p_action:\n",
    "            print(\"Tomando Accion de Politica\")\n",
    "            observation, cost, done, info = env.step(p_action)           \n",
    "            fig=env.render()\n",
    "            s='Pictures/Env' + str(j) + '.png'\n",
    "            fig.savefig(s)\n",
    "        else:\n",
    "            print(\"Usando Heurística\")\n",
    "            action=rollout(env,Heuristic,observation,K,A,N_SAMPLES,vision,0.1)\n",
    "            print(action[0])\n",
    "            observation, cost, done, info = env.step(action[0])\n",
    "            fig=env.render()\n",
    "            s='Pictures/Env' + str(j) + '.png'\n",
    "            fig.savefig(s)\n",
    "        observation_, cost_, done_, info_ = env_1.step(h_action)\n",
    "        total_reward += cost\n",
    "        total_reward_h += cost_\n",
    "        total_reward_step.append(total_reward)\n",
    "        total_reward_step_h.append(total_reward_h)\n",
    "        \n",
    "    print(total_reward)\n",
    "    print(total_reward_h)\n",
    "    env.render()\n",
    "    #End of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    total_reward_step = np.array(total_reward_step)    \n",
    "    total_reward_step_h = np.array(total_reward_step_h) \n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward over time')\n",
    "    plt.plot(total_reward_step ,label='Rollout')\n",
    "    plt.plot(total_reward_step_h, label='Heuristic')\n",
    "    plt.legend()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Q-Table with {0} states\".format(len(policy.policy.keys()))\n",
    "states=policy.policy.keys()\n",
    "f= open(\"Policy_1s_1v.txt\",\"w+\")\n",
    "for state in states:\n",
    "    s+=\"\\n{0} {1} {2} {3} {4}\".format(state,\n",
    "                                  policy.policy[state][0],\n",
    "                                  policy.policy[state][1],\n",
    "                                  policy.policy[state][2],\n",
    "                                  policy.policy[state][3])\n",
    "    f.write(s)\n",
    "    s=\"\"\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
