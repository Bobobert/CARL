{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import helicopter\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as st\n",
    "import time\n",
    "import copy \n",
    "import Heuristic \n",
    "import Rollout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():    \n",
    "    def __init__(self, ):\n",
    "        self.policy = dict()\n",
    "\n",
    "    def new(self,key,control,value):\n",
    "        print(key)\n",
    "        try:\n",
    "            # The key state is already seen\n",
    "            controls, values, actions_freq , total_freq = self.policy[key]\n",
    "            # Iteration over controls available in key until reach it\n",
    "            if control in controls:\n",
    "                print(\"Estado con acción ya escogida\")\n",
    "                print(self.policy[key])\n",
    "                i = 0\n",
    "                for j in controls:\n",
    "                    if j == control:\n",
    "                        break\n",
    "                    else:\n",
    "                        i += 1\n",
    "                values[i]=(values[i]+value)/2\n",
    "                actions_freq[i]+=1                               \n",
    "            #If state has not this control, then add it with actual reward and update freq\n",
    "            else:\n",
    "                print(\"Estado con acción aún no vista\")\n",
    "                print(self.policy[key])\n",
    "                controls.append(control)\n",
    "                values.append(value)\n",
    "                actions_freq.append(1)\n",
    "            total_freq+=1\n",
    "            self.policy[key] = (controls,values, actions_freq, total_freq)\n",
    "            print(self.policy[key])\n",
    "        except:\n",
    "            # The key state is new, so their values are created\n",
    "            #Set of controls, Set of q_values,freq of choosing this control, total freq of being in this state\n",
    "            print(\"Nuevo estado agregado\")            \n",
    "            self.policy[key] = ([control],[value],[1],1) \n",
    "            print(self.policy[key])\n",
    "            \n",
    "    def __repr__(self):\n",
    "        s = \"Q-Table with {0} states\".format(len(self.policy.keys()))\n",
    "        for key in self.policy.keys():\n",
    "            s+=\"\\nState {0} controls:{1}\".format(key,self.policy[key][0])\n",
    "        return s\n",
    "    \n",
    "    def call(self, key,mode):        \n",
    "        try: \n",
    "            print(\"Modo:\",mode)\n",
    "            print(\"State:\",key)\n",
    "            (controls,q_values,freqs,total_freq)= self.policy[key]\n",
    "            print(\"Encontre el estado:\",self.policy[key])\n",
    "            if mode==\"stochastic\":\n",
    "                print(\"Seleccion estocastica de controles\")\n",
    "                freq_r = np.array(freqs) / total_freq\n",
    "                action_max=np.random.choice(controls, 1, p=freq_r)[0]\n",
    "                return action_max\n",
    "            elif mode==\"deterministic\":\n",
    "                print(\"selección determinista de controles\")\n",
    "                action_max= controls[q_values.index(max(q_values))]\n",
    "                return  action_max\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Copy(env):\n",
    "    n_env = helicopter.EnvMakerForestFire(init_pos_row=env.pos_row,init_pos_col=env.pos_col,n_row = env.n_row, n_col = env.n_col,\n",
    "                                          p_tree = env.p_tree, p_fire =env.p_fire, moves_before_updating = env.moves_before_updating,\n",
    "                                          reward_type = env.reward_type, reward_tree = env.reward_tree,reward_fire = env.reward_fire,\n",
    "                                          reward_empty =env.reward_empty, reward_hit = env.reward_hit,sub_tree = env.sub_tree,\n",
    "                                          sub_empty = env.sub_empty, sub_fire = env.sub_fire, sub_rock = env.sub_rock,sub_lake = env.sub_lake,\n",
    "                                          ip_tree = env.ip_tree, ip_empty =env.ip_empty, ip_fire =env.ip_fire, ip_rock = env.ip_rock,\n",
    "                                          ip_lake = env.ip_lake)\n",
    "    n_env.grid = copy.deepcopy(env.grid)      \n",
    "    n_env.total_reward = copy.deepcopy(env.total_reward)   \n",
    "    n_env.total_hits=copy.deepcopy(env.total_hits)\n",
    "    n_env.remaining_moves=copy.deepcopy(env.remaining_moves)\n",
    "    \n",
    "    return n_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "N_ROW = 16\n",
    "N_COL = 16\n",
    "Init_Row=7\n",
    "Init_Col=7\n",
    "P_FIRE = 0.03\n",
    "P_TREE = 0.1\n",
    "# Symbols for cells\n",
    "TREE = 0\n",
    "FIRE = 2\n",
    "EMPTY = 1\n",
    "FREEZE = 8 #Movements of Helicopter after update Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Create a new enviroment with initial parameters\n",
    "    env = helicopter.EnvMakerForestFire(n_row = N_ROW, n_col = N_COL, p_tree = P_TREE, p_fire = P_FIRE,\n",
    "                 init_pos_row = Init_Row, init_pos_col = Init_Col, moves_before_updating = FREEZE,  \n",
    "                 tree = TREE, empty = EMPTY, fire = FIRE)  \n",
    "  \n",
    "    # First observation\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #Create a copy of enviroment with initial observation\n",
    "    env_1 = Copy(env)\n",
    "    observation_1 = observation   \n",
    "    \n",
    "    # Making checkpoints\n",
    "    checkpoint_env = env.make_checkpoint()\n",
    "    checkpoint_env_1 = env_1.make_checkpoint()\n",
    "    \n",
    "    #Create a new empty Policy     \n",
    "    policy= Policy()\n",
    "    \n",
    "    #Rollout Variables\n",
    "    N_TEST= 20      #Number of Training total simulations of Rollout\n",
    "    N_STEPS=50      #Number of steps in rollout(50 updates of enviroment and 20*8 movements of agent)\n",
    "    A=0.9           #Discount factor for future rewards\n",
    "    K=10            #Rollout Steps on horizon for the heuristic \n",
    "    N_SAMPLES=20    #Number of samples trajectories in rollout to calculate expected value\n",
    "    vision= 1       #Range to lookup in cells in helicpter heuristic  \n",
    "    epsilon=0.99    #Epsilon for exploration in state space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    RO_RESULTS=[]\n",
    "    H_RESULTS=[]\n",
    "    RO_RESULTS_C=[]\n",
    "    H_RESULTS_C=[]\n",
    "    start = time.time()\n",
    "    for n_test in range(N_TEST):        \n",
    "        print(\"Test:\",n_test)           \n",
    "        env.load_checkpoint(checkpoint_env)\n",
    "        env_1.load_checkpoint(checkpoint_env_1) \n",
    "        rollout_cost=0\n",
    "        heuristic_cost=0\n",
    "        rollout_cost_step=[]\n",
    "        heuristic_cost_step=[]        \n",
    "        for i in tqdm.tqdm(range(FREEZE * N_STEPS)):\n",
    "            print(\"Step:\",i)\n",
    "            #env.render()\n",
    "            r_action, q_value=Rollout.rollout(env,Heuristic.Heuristic,observation,K,A,N_SAMPLES,vision,epsilon)\n",
    "            epsilon=epsilon*0.99\n",
    "            h_action=Heuristic.Heuristic(observation_1,vision)\n",
    "            #Update Policy            \n",
    "            policy.new(env.Encode(),r_action,q_value)            \n",
    "            ###############\n",
    "\n",
    "            print(\"Rollout Action:\",r_action)\n",
    "            print(\"Heuristic Action:\",h_action)\n",
    "            #Next Steps\n",
    "            observation, ro_cost, _, _ = env.step(r_action)\n",
    "            observation_1, h_cost, _, _ = env_1.step(h_action)  \n",
    "\n",
    "            rollout_cost += ro_cost  #Acumulative cost for rollout          \n",
    "            rollout_cost_step.append(rollout_cost)  #List of cost over time\n",
    "\n",
    "            heuristic_cost += h_cost\n",
    "            heuristic_cost_step.append(heuristic_cost)\n",
    "\n",
    "            print(\"Rollout in step {} is: {}\".format(i,rollout_cost))\n",
    "            print(\"Heuristic in step {} is: {}\".format(i,heuristic_cost))\n",
    "        print(\"Rollout:\",rollout_cost)\n",
    "        print(\"heuristic:\",heuristic_cost)\n",
    "        #Costs p/test\n",
    "        RO_RESULTS.append(rollout_cost)             \n",
    "        H_RESULTS.append(heuristic_cost)\n",
    "        #Cumulative costs p/test\n",
    "        RO_RESULTS_C.append(rollout_cost_step)\n",
    "        H_RESULTS_C.append(heuristic_cost_step)\n",
    "    print(\"Total time execution %.3f s\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #OBTAIN AVERAGE COSTS PER STAGE\n",
    "    ITER=[]\n",
    "    ITER2=[]\n",
    "    RO_RESULTS_C=np.array(RO_RESULTS_C)    \n",
    "    for i in range(RO_RESULTS_C.shape[1]):\n",
    "        IT=[]\n",
    "        IT2=[]\n",
    "        for j in range(RO_RESULTS_C.shape[0]):\n",
    "            IT.append(RO_RESULTS_C[j][i])\n",
    "            IT2.append(H_RESULTS_C[j][i])\n",
    "        ITER.append(st.mean(IT))\n",
    "        ITER2.append(st.mean(IT2))        \n",
    "    \n",
    "    x = np.arange(RO_RESULTS_C.shape[1])    \n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average reward over 20 test (1 step-1 vision)')\n",
    "    plt.plot(x,ITER ,label='Rollout')\n",
    "    plt.plot(x,ITER2, label='Heuristic')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Graph of total reward by test\n",
    "    RO_RESULTS = np.array(RO_RESULTS)    \n",
    "    H_RESULTS = np.array(H_RESULTS) \n",
    "    RO_RESULTS_MEAN=[]    \n",
    "    RO_RESULTS_MEAN.append(st.mean(np.array(RO_RESULTS_C[0])))   \n",
    "    x = np.arange(20)    \n",
    "    plt.xlabel('Test')\n",
    "    plt.ylabel('Final Total Reward (1 step- 1 vision)')\n",
    "    plt.title('Rollout Tests')\n",
    "    plt.plot(x,RO_RESULTS ,label='Rollout')\n",
    "    plt.plot(x,H_RESULTS, label='Heuristic')\n",
    "    plt.legend()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Simulation of enviroment with Trained Policy    \n",
    "    observation = env.reset()\n",
    "    env_1 = env.Copia()\n",
    "    observation_=observation\n",
    "    fig=env.render()\n",
    "    fig.savefig('Pictures/Env.png')\n",
    "    total_reward = 0\n",
    "    total_reward_step=[]\n",
    "    total_reward_h = 0\n",
    "    total_reward_step_h=[]   \n",
    "    N_STEPS=50\n",
    "    for j in tqdm.tqdm(range(N_STEPS*FREEZE)):\n",
    "        p_action=policy.call(env.Encode(),\"stochastic\")\n",
    "        print(env.Encode())\n",
    "        h_action=Heuristic(observation_,vision)\n",
    "        if p_action:\n",
    "            print(\"Tomando Accion de Politica\")\n",
    "            observation, cost, done, info = env.step(p_action)           \n",
    "            fig=env.render()\n",
    "            s='Pictures/Env' + str(j) + '.png'\n",
    "            fig.savefig(s)\n",
    "        else:\n",
    "            print(\"Usando Heurística\")\n",
    "            action=rollout(env,Heuristic,observation,K,A,N_SAMPLES,vision,0.1)\n",
    "            print(action[0])\n",
    "            observation, cost, done, info = env.step(action[0])\n",
    "            fig=env.render()\n",
    "            s='Pictures/Env' + str(j) + '.png'\n",
    "            fig.savefig(s)\n",
    "        observation_, cost_, done_, info_ = env_1.step(h_action)\n",
    "        total_reward += cost\n",
    "        total_reward_h += cost_\n",
    "        total_reward_step.append(total_reward)\n",
    "        total_reward_step_h.append(total_reward_h)\n",
    "        \n",
    "    print(total_reward)\n",
    "    print(total_reward_h)\n",
    "    env.render()\n",
    "    #End of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    total_reward_step = np.array(total_reward_step)    \n",
    "    total_reward_step_h = np.array(total_reward_step_h) \n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward over time')\n",
    "    plt.plot(total_reward_step ,label='Rollout')\n",
    "    plt.plot(total_reward_step_h, label='Heuristic')\n",
    "    plt.legend()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    s = \"Q-Table with {0} states\".format(len(policy.policy.keys()))\n",
    "    states=policy.policy.keys()\n",
    "    f= open(\"Policy_1s_1v.txt\",\"w+\")\n",
    "    for state in states:\n",
    "    s+=\"\\n{0} {1} {2} {3} {4}\".format(state,\n",
    "                                  policy.policy[state][0],\n",
    "                                  policy.policy[state][1],\n",
    "                                  policy.policy[state][2],\n",
    "                                  policy.policy[state][3])\n",
    "    f.write(s)\n",
    "    s=\"\"\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
